name: CD

on:
  workflow_run:
    workflows: ["CI"]
    types:
      - completed
    branches:
      - master

permissions:
  id-token: write
  contents: read

env:
  AWS_REGION: ${{ secrets.AWS_REGION }}
  AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }}

jobs:
  prepare-deployment:
    runs-on: ubuntu-latest
    if: ${{ github.event.workflow_run.conclusion == 'success' }}
    steps:
      - uses: actions/checkout@v4
      
      - name: Download artifact
        uses: actions/download-artifact@v4
        with:
          name: build-artifacts
          path: .
          github-token: ${{ secrets.GITHUB_TOKEN }}
          run-id: ${{ github.event.workflow_run.id }}
      
      - name: Display structure of downloaded files
        run: ls -R
        
      - name: Upload artifacts for next jobs
        uses: actions/upload-artifact@v4
        with:
          name: deployment-artifacts
          path: |
            **/target/*.jar
            rrtb_infra/cdk.out
            rrtb_infra/package.json
            rrtb_infra/package-lock.json

  update-templates:
    needs: deploy-prod
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 2
          ref: ${{ github.event.workflow_run.head_branch }}
      
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Check for template changes
        id: check_changes
        run: |
          echo "Checking all local templates against S3 and comparing checksums..."
          # Ensure AWS CLI output is JSON for easier parsing
          export AWS_PAGER=""
          export AWS_DEFAULT_OUTPUT=json
          
          # Find all files in the local templates directory
          LOCAL_TEMPLATES=$(find templates/ -type f || true)
          FILES_TO_UPLOAD=""

          if [ -z "$LOCAL_TEMPLATES" ]; then
            echo "No template files found in the local templates/ directory."
          else
            echo "Found local template files:"
            echo "$LOCAL_TEMPLATES"
            
            for template in $LOCAL_TEMPLATES; do
              echo "Checking file: $template"
              
              # Check if file exists locally before hashing (redundant with find, but safe)
              if [ ! -f "$template" ]; then
                echo "Warning: File $template found by find but doesn't exist? Skipping."
                continue
              fi

              # Calculate local MD5 checksum
              LOCAL_MD5=$(md5sum "$template" | cut -d' ' -f1)
              echo "Local MD5: $LOCAL_MD5"
              
              # Construct the S3 key (assuming it mirrors the local path)
              S3_KEY="$template"

              # Get S3 ETag (MD5) and handle errors
              echo "Templates bucket: ${{ secrets.TEMPLATES_BUCKET }}"
              S3_METADATA=$(aws s3api head-object --bucket "${{ secrets.TEMPLATES_BUCKET }}" --key "$S3_KEY" 2>&1)
              S3_HEAD_EXIT_CODE=$?
              
              UPLOAD_NEEDED=false
              if [ $S3_HEAD_EXIT_CODE -ne 0 ]; then
                # Check specifically for 'Not Found' or 'Forbidden' errors which indicate upload is needed
                if echo "$S3_METADATA" | grep -q -E '(Not Found|Forbidden|404|403)'; then
                  echo "File not found in S3 or access denied (assuming new file/permission issue). Needs upload."
                  UPLOAD_NEEDED=true
                else
                  echo "Error getting S3 head-object for $S3_KEY: $S3_METADATA"
                  # Decide how to handle other errors - Could skip, or force upload. Forcing upload is safer.
                  echo "Assuming upload needed due to unexpected S3 error."
                  UPLOAD_NEEDED=true 
                fi
              else
                # Extract ETag and remove quotes
                S3_ETAG_RAW=$(echo "$S3_METADATA" | jq -r '.ETag // empty')
                if [ -z "$S3_ETAG_RAW" ]; then
                   echo "Warning: Could not extract ETag from S3 metadata for $S3_KEY. Assuming upload needed."
                   UPLOAD_NEEDED=true
                else
                  S3_ETAG=${S3_ETAG_RAW//\\\"/} # Remove escaped quotes
                  S3_ETAG=${S3_ETAG//\"/}   # Remove normal quotes
                  echo "S3 ETag: $S3_ETAG"
                
                  # Compare checksums
                  if [ "$LOCAL_MD5" != "$S3_ETAG" ]; then
                    echo "Checksums differ. Needs upload."
                    UPLOAD_NEEDED=true
                  else
                    echo "Checksums match. No upload needed."
                  fi
                fi
              fi
              
              if [ "$UPLOAD_NEEDED" = true ]; then
                # Add file to the list, ensuring space separation
                if [ -z "$FILES_TO_UPLOAD" ]; then
                  FILES_TO_UPLOAD="$template"
                else
                  FILES_TO_UPLOAD="$FILES_TO_UPLOAD $template"
                fi
              fi
              echo "---" # Separator for clarity
            done
          fi

          echo "Final list of files needing upload: '$FILES_TO_UPLOAD'"
          echo "files_to_upload=$FILES_TO_UPLOAD" >> $GITHUB_OUTPUT
          
          # Output a boolean flag for easier 'if' condition
          if [ -n "$FILES_TO_UPLOAD" ]; then
            echo "needs_upload=true" >> $GITHUB_OUTPUT
          else
            echo "needs_upload=false" >> $GITHUB_OUTPUT
          fi

      - name: Upload changed templates to S3
        # Use the boolean flag for the condition
        if: steps.check_changes.outputs.needs_upload == 'true'
        run: |
          echo "Uploading templates with differing checksums or not found in S3..."
          # Iterate through the space-separated list from the previous step's output
          for template in ${{ steps.check_changes.outputs.files_to_upload }}; do
            echo "Uploading $template to S3..."
            # Use the same bucket secret and ensure path matches
            aws s3 cp "$template" "s3://${{ secrets.TEMPLATES_BUCKET }}/$template"
          done
          echo "Upload complete."

  deploy-dev:
    needs: prepare-deployment
    runs-on: ubuntu-latest
    environment: development
    steps:
      - uses: actions/checkout@v4
      
      - name: Download deployment artifacts  
        uses: actions/download-artifact@v4
        with:
          name: deployment-artifacts
          path: .
          
      - uses: actions/setup-node@v4
        with:
          node-version: '20'
          
      - name: Install CDK Dependencies
        run: |
          cd rrtb_infra
          npm install
          
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
          
      - name: Update Policy Files
        run: |
          cd rrtb_infra
          sed -i "s/\${AWS_ACCOUNT_ID}/${{ env.AWS_ACCOUNT_ID }}/g" github-actions-policy.json
          sed -i "s/\${TEMPLATES_BUCKET}/${{ secrets.TEMPLATES_BUCKET }}/g" github-actions-policy.json
          
      - name: Bootstrap CDK
        run: |
          cd rrtb_infra
          npx cdk bootstrap
          
      - name: Deploy Application
        run: |
          cd rrtb_infra
          npx cdk deploy --require-approval never

  deploy-test:
    needs: deploy-dev
    runs-on: ubuntu-latest
    environment: test
    steps:
      - uses: actions/checkout@v4
      
      - name: Download deployment artifacts  
        uses: actions/download-artifact@v4
        with:
          name: deployment-artifacts
          path: .
          
      - uses: actions/setup-node@v4
        with:
          node-version: '20'
          
      - name: Install CDK Dependencies
        run: |
          cd rrtb_infra
          npm install
          
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
          
      - name: Update Policy Files
        run: |
          cd rrtb_infra
          sed -i "s/\${AWS_ACCOUNT_ID}/${{ env.AWS_ACCOUNT_ID }}/g" github-actions-policy.json
          
      - name: Bootstrap CDK
        run: |
          cd rrtb_infra
          npx cdk bootstrap
          
      - name: Deploy Application
        run: |
          cd rrtb_infra
          npx cdk deploy --require-approval never

  deploy-prod:
    needs: deploy-test
    runs-on: ubuntu-latest
    environment: production
    steps:
      - uses: actions/checkout@v4
      
      - name: Download deployment artifacts  
        uses: actions/download-artifact@v4
        with:
          name: deployment-artifacts
          path: .
          
      - uses: actions/setup-node@v4
        with:
          node-version: '20'
          
      - name: Install CDK Dependencies
        run: |
          cd rrtb_infra
          npm install
          
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
          
      - name: Update Policy Files
        run: |
          cd rrtb_infra
          sed -i "s/\${AWS_ACCOUNT_ID}/${{ env.AWS_ACCOUNT_ID }}/g" github-actions-policy.json
          
      - name: Bootstrap CDK
        run: |
          cd rrtb_infra
          npx cdk bootstrap
          
      - name: Deploy Application
        run: |
          cd rrtb_infra
          npx cdk deploy --require-approval never 